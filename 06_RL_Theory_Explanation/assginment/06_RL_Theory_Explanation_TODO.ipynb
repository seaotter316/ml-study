{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8203e2e3",
   "metadata": {},
   "source": [
    "# Cliff Walking: SARSA vs Q-learning\n",
    "\n",
    "생각해보니까 환경을 전에 썼던 `gymnasium`로 환경 구성 날먹하는건 학습에 안 좋을것 같으니까<br>\n",
    "내가 만들어놓은 환경 코드를 읽고 어떻게 쓰는지 유추해보쇼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095791e5",
   "metadata": {},
   "source": [
    "## 1. 환경 정의 (CliffWalking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쭉 읽고 해석해보셈\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, n_rows: int = 4, n_cols: int = 12):\n",
    "        # TODO: 아래 기본 모양에서 start, goal, cliff가 어딘지 표기하시오\n",
    "        #\n",
    "        #             기본 모양\n",
    "        #               column\n",
    "        #        0 1 2 3 4 5 6 7 8 9 10 11\n",
    "        #        _________________________\n",
    "        #     0 |                         |\n",
    "        # row 1 |                         |\n",
    "        #     2 |                         |\n",
    "        #     3 |                         |\n",
    "        #       ￣￣￣￣￣￣￣￣￣￣￣￣￣￣\n",
    "\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_states = n_rows * n_cols\n",
    "        self.n_actions = 4\n",
    "\n",
    "        self.start = (n_rows - 1, 0)\n",
    "        self.goal = (n_rows - 1, n_cols - 1)\n",
    "        self.cliff = {(n_rows - 1, c) for c in range(1, n_cols - 1)}\n",
    "        self.reset()\n",
    "    \n",
    "    # 아래 python의 언더바 문법은 이 주소를 참고: https://velog.io/@turningtwenty/underscore-in-python\n",
    "    def _to_state(self, rc: tuple[int, int]) -> int:\n",
    "        r, c = rc\n",
    "        return r * self.n_cols + c\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        self.agent_rc = self.start\n",
    "        return self._to_state(self.agent_rc)\n",
    "\n",
    "    # next_state, reward, done, info를 return\n",
    "    def step(self, action: int) -> tuple[int, float, bool, dict]:\n",
    "        r, c = self.agent_rc\n",
    "\n",
    "        if action == 0:\n",
    "            r2, c2 = r - 1, c\n",
    "        elif action == 1:\n",
    "            r2, c2 = r, c + 1\n",
    "        elif action == 2:\n",
    "            r2, c2 = r + 1, c\n",
    "        elif action == 3:\n",
    "            r2, c2 = r, c - 1\n",
    "        else:\n",
    "            raise ValueError(\"action은 0, 1, 2, 3 중 하나여야 합니다.\")\n",
    "\n",
    "        # 경계 처리\n",
    "        r2 = min(max(r2, 0), self.n_rows - 1)\n",
    "        c2 = min(max(c2, 0), self.n_cols - 1)\n",
    "\n",
    "        next_rc = (r2, c2)\n",
    "        reward = -1.0\n",
    "        done = False\n",
    "\n",
    "        # 절벽\n",
    "        # TODO: 절벽에서 떨어졌을 때 done이 False인건 Q-learning과 SARSA의 차이를 만들기 위함인데,\n",
    "        #       done이 False일 때 왜 Q-learning과 SARSA간 차이가 커지는지 설명하시오\n",
    "        #\n",
    "        # 답: \n",
    "        #    \n",
    "        #    \n",
    "        \n",
    "        if next_rc in self.cliff:\n",
    "            reward = -100.0\n",
    "            next_rc = self.start\n",
    "            done = False\n",
    "\n",
    "        # 목표 지점\n",
    "        if next_rc == self.goal:\n",
    "            done = True\n",
    "\n",
    "        self.agent_rc = next_rc\n",
    "        return self._to_state(next_rc), reward, done, {}\n",
    "\n",
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbda268",
   "metadata": {},
   "source": [
    "## 2. 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARROWS = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
    "\n",
    "# 아래 함수들은 시각화 용이니까 무시\n",
    "def moving_average(x, w: int = 20):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    kernel = np.ones(w) / w\n",
    "    return np.convolve(x, kernel, mode=\"valid\")\n",
    "\n",
    "def render_policy(Q: np.ndarray, env: CliffWalkingEnv):\n",
    "    grid = []\n",
    "    for r in range(env.n_rows):\n",
    "        row = []\n",
    "        for c in range(env.n_cols):\n",
    "            rc = (r, c)\n",
    "            if rc == env.start:\n",
    "                row.append(\"S\")\n",
    "            elif rc == env.goal:\n",
    "                row.append(\"G\")\n",
    "            elif rc in env.cliff:\n",
    "                row.append(\"C\")\n",
    "            else:\n",
    "                s = r * env.n_cols + c\n",
    "                a = int(np.argmax(Q[s]))\n",
    "                row.append(ARROWS[a])\n",
    "        grid.append(\" \".join(row))\n",
    "    print(\"\\n\".join(grid))\n",
    "\n",
    "def run_greedy_episode(Q: np.ndarray, env: CliffWalkingEnv, max_steps: int = 200):\n",
    "    s = env.reset()\n",
    "    total = 0.0\n",
    "    for _ in range(max_steps):\n",
    "        a = int(np.argmax(Q[s]))\n",
    "        s2, r, done, _ = env.step(a)\n",
    "        total += r\n",
    "        s = s2\n",
    "        if done:\n",
    "            break\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b8c9b",
   "metadata": {},
   "source": [
    "## 3. 구현: ε-greedy / SARSA / Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state, epsilon, n_actions):\n",
    "    # TODO: epsilon-greedy 정책을 구현하시오\n",
    "    pass\n",
    "\n",
    "def train_sarsa(\n",
    "    env: CliffWalkingEnv,\n",
    "    num_episodes: int = 500,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 1.0,\n",
    "    epsilon_decay: float = 0.995,\n",
    "    epsilon_min: float = 0.05,\n",
    "    max_steps: int = 500,\n",
    "):\n",
    "    # TODO: Q 값을 초기화하시오\n",
    "    Q = pass\n",
    "    returns = np.zeros(num_episodes, dtype=float)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        a = epsilon_greedy(Q, state, epsilon, env.n_actions)\n",
    "        total = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # TODO: SARSA 알고리즘을 구현하시오\n",
    "            pass\n",
    "\n",
    "        returns[ep] = total\n",
    "        # TODO: epsilon 값을 decay 시키시오\n",
    "        pass\n",
    "\n",
    "    return Q, returns\n",
    "\n",
    "\n",
    "\n",
    "def train_q_learning(\n",
    "    env: CliffWalkingEnv,\n",
    "    num_episodes: int = 500,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 1.0,\n",
    "    epsilon_decay: float = 0.995,\n",
    "    epsilon_min: float = 0.05,\n",
    "    max_steps: int = 500,\n",
    "):\n",
    "    # TODO: Q 값을 초기화하시오\n",
    "    Q = \n",
    "    returns = np.zeros(num_episodes, dtype=float)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # TODO: Q-learning 알고리즘을 구현하시오\n",
    "            pass\n",
    "\n",
    "        returns[ep] = total\n",
    "        # TODO: epsilon 값을 decay 시키시오\n",
    "        pass\n",
    "\n",
    "    return Q, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec1aa9",
   "metadata": {},
   "source": [
    "## 4. 실행 & 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ed902",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "num_episodes = 500\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.05\n",
    "\n",
    "Q_sarsa, ret_sarsa = train_sarsa(env, num_episodes=num_episodes, alpha=alpha, gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min)\n",
    "Q_ql,    ret_ql    = train_q_learning(env, num_episodes=num_episodes, alpha=alpha, gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(moving_average(ret_sarsa, w=20), label=\"SARSA (moving avg)\")\n",
    "plt.plot(moving_average(ret_ql, w=20), label=\"Q-learning (moving avg)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== SARSA greedy policy ===\")\n",
    "render_policy(Q_sarsa, env)\n",
    "print(\"\\n=== Q-learning greedy policy ===\")\n",
    "render_policy(Q_ql, env)\n",
    "\n",
    "eval_sarsa = np.mean([run_greedy_episode(Q_sarsa, env) for _ in range(50)])\n",
    "eval_ql    = np.mean([run_greedy_episode(Q_ql, env) for _ in range(50)])\n",
    "print(f\"\\nGreedy eval (50 eps): SARSA={eval_sarsa:.1f}, QL={eval_ql:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e283209",
   "metadata": {},
   "source": [
    "# 5. 추가 분석들\n",
    "위 코드를 여러번 복붙해서 다른 셀에 epsilon decay 없는 경우를 작성하고, 해당 epsilon의 값을 여러개로 바꾸어가며 여러 셀에서 실행하여 비교하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a9d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852b0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6ec41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0e355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c0506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
