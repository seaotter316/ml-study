{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fdb95",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Conv(1→16) → ReLU → Pool → Conv(16→32) → ReLU → Pool → Flatten → Linear → ReLU → Linear\n",
    "- Conv는 kernel=3, stride=1, padding=1\n",
    "- Pool은 MaxPool2d(2)\n",
    "\n",
    "- 입력: (B, 1, 28, 28)\n",
    "- Conv1 → (B, 16, 28, 28)\n",
    "- Pool1 → (B, 16, 14, 14)\n",
    "- Conv2 → (B, 32, 14, 14)\n",
    "- Pool2 → (B, 32, 7, 7)\n",
    "- Flatten → (B, 32 × 7 × 7)\n",
    "- Linear1 input dim = 32 × 7 × 7 = 1568"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac5346",
   "metadata": {},
   "source": [
    "---\n",
    "## Fashion-MNIST 실험\n",
    "\n",
    "### 실험 목표\n",
    "- SmallCNN vs BigCNN으로 **과적합 유도**\n",
    "- Dropout/Weight Decay로 **일반화 성능 변화 체감**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "fashion_classes = train_dataset.classes\n",
    "print(\"classes:\", fashion_classes[:5], \"... total:\", len(fashion_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158103a",
   "metadata": {},
   "source": [
    "### 샘플 16장 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c592d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4x4 시각화\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images[:16]\n",
    "labels = labels[:16]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    # Normalize 되돌리기: x_norm = (x - 0.5)/0.5 -> x = x_norm*0.5 + 0.5\n",
    "    img = images[i].squeeze().numpy() * 0.5 + 0.5\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(fashion_classes[labels[i].item()], fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d76438",
   "metadata": {},
   "source": [
    "### 모델 정의: SmallCNN / BigCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 28 -> 14\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 14 -> 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class BigCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 28 -> 14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 14 -> 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "small = SmallCNN()\n",
    "big = BigCNN()\n",
    "\n",
    "print(\"SmallCNN params:\", count_params(small))\n",
    "print(\"BigCNN   params:\", count_params(big))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380affd",
   "metadata": {},
   "source": [
    "### 빠른 학습: Small vs Big (2~3 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    \"\"\"logits: (B, C), y: (B,)\"\"\"\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy_from_logits(logits, y)\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1), total_acc / max(n_batches, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy_from_logits(logits, y)\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1), total_acc / max(n_batches, 1)\n",
    "\n",
    "\n",
    "def run_training(model, train_loader, test_loader, epochs=2, lr=1e-3, weight_decay=0.0):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        te_loss, te_acc = evaluate(model, test_loader, criterion, device)\n",
    "        history.append((tr_loss, tr_acc, te_loss, te_acc))\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | test loss {te_loss:.4f} acc {te_acc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_FASHION = 2\n",
    "lr = 1e-3\n",
    "\n",
    "print(\"\\n[Train] SmallCNN\")\n",
    "hist_small = run_training(SmallCNN(), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n",
    "\n",
    "print(\"\\n[Train] BigCNN\")\n",
    "hist_big = run_training(BigCNN(), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n",
    "\n",
    "# 최종 성능 요약\n",
    "small_tr_acc = hist_small[-1][1]\n",
    "small_te_acc = hist_small[-1][3]\n",
    "big_tr_acc   = hist_big[-1][1]\n",
    "big_te_acc   = hist_big[-1][3]\n",
    "\n",
    "print(\"\\nSummary (last epoch)\")\n",
    "print(f\"Small: train acc={small_tr_acc:.4f} test acc={small_te_acc:.4f}\")\n",
    "print(f\"Big  : train acc={big_tr_acc:.4f} test acc={big_te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f8de5",
   "metadata": {},
   "source": [
    "### BigCNN + Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigCNN_Dropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 28 -> 14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 14 -> 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"\\n[Train] BigCNN + Dropout\")\n",
    "hist_big_do = run_training(BigCNN_Dropout(p=0.5), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n",
    "\n",
    "big_do_tr_acc = hist_big_do[-1][1]\n",
    "big_do_te_acc = hist_big_do[-1][3]\n",
    "\n",
    "print(\"\\nSummary (last epoch)\")\n",
    "print(f\"Big+DO: train acc={big_do_tr_acc:.4f} test acc={big_do_te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ead50",
   "metadata": {},
   "source": [
    "### Weight Decay (Big + Dropout + WD)\n",
    "\n",
    "Weight Decay는 파라미터 크기를 너무 키우지 않도록 벌점을 주는 방식으로 과적합을 완화\n",
    "https://eair.tistory.com/80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da98c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WD = 1e-4\n",
    "print(\"\\n[Train] BigCNN + Dropout + Weight Decay\")\n",
    "hist_big_do_wd = run_training(BigCNN_Dropout(p=0.5), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr, weight_decay=WD)\n",
    "\n",
    "big_do_wd_tr_acc = hist_big_do_wd[-1][1]\n",
    "big_do_wd_te_acc = hist_big_do_wd[-1][3]\n",
    "\n",
    "print(\"\\nSummary (last epoch)\")\n",
    "print(f\"Big+DO+WD: train acc={big_do_wd_tr_acc:.4f} test acc={big_do_wd_te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779d051",
   "metadata": {},
   "source": [
    "### 결과 표 정리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad407eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_fashion = pd.DataFrame({\n",
    "    \"Model\": [\"Small\", \"Big\", \"Big + Dropout\", \"Big + Dropout + WD\"],\n",
    "    \"Train Acc (last)\": [small_tr_acc, big_tr_acc, big_do_tr_acc, big_do_wd_tr_acc],\n",
    "    \"Test Acc (last)\":  [small_te_acc, big_te_acc, big_do_te_acc, big_do_wd_te_acc],\n",
    "})\n",
    "results_fashion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693160c",
   "metadata": {},
   "source": [
    "---\n",
    "## CIFAR-10 프로젝트 맛보기\n",
    "\n",
    "- 입력이 (B, 3, 32, 32)로 바뀜.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2f085",
   "metadata": {},
   "source": [
    "### CIFAR-10 DataLoader 세팅 + 16장 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2174bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_cifar = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_cifar = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform_cifar)\n",
    "test_cifar  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform_cifar)\n",
    "\n",
    "train_loader_cifar = DataLoader(train_cifar, batch_size=64, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "test_loader_cifar  = DataLoader(test_cifar, batch_size=1000, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "cifar_classes = train_cifar.classes\n",
    "print(\"cifar classes:\", cifar_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48111b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 16장 시각화 (Normalize 되돌린 뒤 표시)\n",
    "images, labels = next(iter(train_loader_cifar))\n",
    "images = images[:16]\n",
    "labels = labels[:16]\n",
    "\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "std  = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    img = images[i].cpu() * std + mean\n",
    "    img = img.permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(cifar_classes[labels[i].item()], fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6aaa5",
   "metadata": {},
   "source": [
    "### CIFAR용 CNN 모델\n",
    "\n",
    "32×32 입력에서 Pool(2) 두 번이면 32→16→8이 되어 Flatten은 (채널 * 8 * 8)이 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARCNN(nn.Module):\n",
    "    def __init__(self, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),     # 32 -> 16\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),     # 16 -> 8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"CIFARCNN params:\", count_params(CIFARCNN()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8401b",
   "metadata": {},
   "source": [
    "### 1~2 epoch 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f999a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_CIFAR = 2\n",
    "\n",
    "print(\"\\n[Train] CIFARCNN baseline\")\n",
    "hist_cifar = run_training(CIFARCNN(dropout_p=0.5), train_loader_cifar, test_loader_cifar, epochs=EPOCHS_CIFAR, lr=lr)\n",
    "\n",
    "cifar_tr_acc = hist_cifar[-1][1]\n",
    "cifar_te_acc = hist_cifar[-1][3]\n",
    "print(f\"Baseline CIFAR: train acc={cifar_tr_acc:.4f} test acc={cifar_te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCIFAR-10 baseline:\")\n",
    "pd.DataFrame({\n",
    "    \"Model\": [\"CIFARCNN baseline\"],\n",
    "    \"Train Acc (last)\": [cifar_tr_acc],\n",
    "    \"Test Acc (last)\": [cifar_te_acc],\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
