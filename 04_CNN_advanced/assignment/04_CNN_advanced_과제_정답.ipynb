{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aabe71",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) 환경 설정 & 공통 유틸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits, y):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy_from_logits(logits, y)\n",
    "        n_batches += 1\n",
    "    return total_loss / max(n_batches, 1), total_acc / max(n_batches, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy_from_logits(logits, y)\n",
    "        n_batches += 1\n",
    "    return total_loss / max(n_batches, 1), total_acc / max(n_batches, 1)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def run_training(model, train_loader, test_loader, epochs, lr, weight_decay=0.0):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        te_loss, te_acc = evaluate(model, test_loader, criterion, device)\n",
    "        history.append((tr_loss, tr_acc, te_loss, te_acc))\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | Train acc {tr_acc:.4f} | Test acc {te_acc:.4f}\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110f411",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Fashion-MNIST: 데이터 로더\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "test_dataset  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "fashion_classes = train_dataset.classes\n",
    "print(\"Fashion classes:\", fashion_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d94641",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Fashion-MNIST: 모델 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3068461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 28 -> 14\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 14 -> 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class BigCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 28 -> 14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 14 -> 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class BigCNN_Dropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 28 -> 14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 14 -> 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"Small params:\", count_params(SmallCNN()))\n",
    "print(\"Big   params:\", count_params(BigCNN()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeec89",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Fashion-MNIST: 본 실험 (epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7817bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_FASHION = 10\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "\n",
    "print(\"\\n[Train] SmallCNN\")\n",
    "hist_small = run_training(SmallCNN(), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n",
    "\n",
    "print(\"\\n[Train] BigCNN\")\n",
    "hist_big = run_training(BigCNN(), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n",
    "\n",
    "print(\"\\n[Train] BigCNN + Dropout(p=0.5)\")\n",
    "hist_big_do = run_training(BigCNN_Dropout(p=0.5), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n",
    "\n",
    "print(\"\\n[Train] BigCNN + Dropout(p=0.5) + WD(1e-4)\")\n",
    "hist_big_do_wd = run_training(BigCNN_Dropout(p=0.5), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr, weight_decay=wd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Train] Extra: BigCNN + Dropout(p=0.7)\")\n",
    "hist_extra = run_training(BigCNN_Dropout(p=0.7), train_loader, test_loader, epochs=EPOCHS_FASHION, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f153a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_acc(hist):\n",
    "    return hist[-1][1], hist[-1][3]\n",
    "\n",
    "rows = [\n",
    "    (\"SmallCNN\", \"-\", *last_acc(hist_small)),\n",
    "    (\"BigCNN\", \"channels/linear↑\", *last_acc(hist_big)),\n",
    "    (\"BigCNN+DO\", \"dropout p=0.5\", *last_acc(hist_big_do)),\n",
    "    (\"BigCNN+DO+WD\", \"dropout p=0.5, wd=1e-4\", *last_acc(hist_big_do_wd)),\n",
    "    (\"Extra\", \"dropout p=0.7\", *last_acc(hist_extra)),\n",
    "]\n",
    "results_fashion = pd.DataFrame(rows, columns=[\"Model\", \"Change\", \"TrainAcc_last\", \"TestAcc_last\"])\n",
    "results_fashion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99775031",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train = [x[1] for x in hist_big]\n",
    "big_test  = [x[3] for x in hist_big]\n",
    "do_train  = [x[1] for x in hist_big_do]\n",
    "do_test   = [x[3] for x in hist_big_do]\n",
    "epochs = list(range(1, EPOCHS_FASHION + 1))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(epochs, big_train, label=\"Big train\")\n",
    "plt.plot(epochs, big_test, label=\"Big test\")\n",
    "plt.plot(epochs, do_train, label=\"Big+DO train\")\n",
    "plt.plot(epochs, do_test, label=\"Big+DO test\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Fashion-MNIST: Big vs Big+Dropout\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751bff0",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) CIFAR-10: 데이터 로더\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_cifar = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "test_transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_cifar = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform_cifar)\n",
    "test_cifar  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform_cifar)\n",
    "\n",
    "train_loader_cifar = DataLoader(train_cifar, batch_size=64, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "test_loader_cifar  = DataLoader(test_cifar, batch_size=1000, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "cifar_classes = train_cifar.classes\n",
    "print(\"cifar classes:\", cifar_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a04a8c9",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) CIFAR-10: baseline + 개선 모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARCNN(nn.Module):\n",
    "    def __init__(self, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),     # 32 -> 16\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),     # 16 -> 8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"CIFARCNN params:\", count_params(CIFARCNN()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e556e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_CIFAR = 10\n",
    "lr_cifar = 1e-3\n",
    "\n",
    "print(\"\\n[Train] CIFAR baseline\")\n",
    "hist_cifar_base = run_training(CIFARCNN(dropout_p=0.5),\n",
    "                               train_loader_cifar, test_loader_cifar,\n",
    "                               epochs=EPOCHS_CIFAR, lr=lr_cifar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd47dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Train] CIFAR improved (channels↑ + wd)\")\n",
    "hist_cifar_improved = run_training(CIFARCNN(dropout_p=0.5),\n",
    "                                   train_loader_cifar, test_loader_cifar,\n",
    "                                   epochs=EPOCHS_CIFAR, lr=lr_cifar, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610794c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_te = hist_cifar_base[-1][3]\n",
    "impr_te = hist_cifar_improved[-1][3]\n",
    "results_cifar = pd.DataFrame([\n",
    "    (\"Baseline\", base_te),\n",
    "    (\"Improved\", impr_te),\n",
    "], columns=[\"Model\", \"Change\", \"TestAcc_last\"])\n",
    "results_cifar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ccfab",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) 개념 요약 (각 3줄 이내, 과제 결과 예시 포함)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50774c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "overfitting = \"\"\"\n",
    "BigCNN에서 epoch를 늘리면 train acc는 계속 오르는데 test acc가 정체/하락하는 현상이 나타날 수 있다.\n",
    "이때 train/test 격차가 커지는 것이 과적합의 대표적인 신호다.\n",
    "(예: BigCNN의 epoch 6~10 구간에서 train↑, test↔/↓)\n",
    "\"\"\"\n",
    "\n",
    "dropout = \"\"\"\n",
    "학습 중 일부 뉴런 출력을 확률적으로 0으로 만들어 특정 경로 의존을 줄이는 정규화다.\n",
    "BigCNN+Dropout은 보통 train acc를 낮추지만 test acc를 유지/개선해 일반화에 도움을 줄 수 있다.\n",
    "train()에서만 적용되고 eval()에서는 꺼진다.\n",
    "\"\"\"\n",
    "\n",
    "weight_decay = \"\"\"\n",
    "loss에 L2 패널티를 더해 weight가 과도하게 커지는 것을 억제하는 정규화다.\n",
    "보통 train 성능을 약간 희생해 test 성능을 안정화시키는 방향으로 작동한다.\n",
    "(예: BigCNN+Dropout+WD가 BigCNN보다 test 추세가 덜 흔들림)\n",
    "\"\"\"\n",
    "\n",
    "augmentation = \"\"\"\n",
    "훈련 데이터에 랜덤 변형(Flip/Crop 등)을 줘서 데이터 다양성을 늘리는 기법이다.\n",
    "CIFAR-10처럼 실제 이미지에서 위치/방향 변화에 덜 민감하게 만들어 일반화를 돕는다.\n",
    "테스트에는 랜덤성을 넣지 않는다.\n",
    "\"\"\"\n",
    "\n",
    "print(overfitting)\n",
    "print(dropout)\n",
    "print(weight_decay)\n",
    "print(augmentation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
